{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import docx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, expon\n",
    "from statistics import mean, stdev\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define QueryPerformanceAnalyzer class\n",
    "class QueryPerformanceAnalyzer:\n",
    "    def __init__(self, query_file_path):\n",
    "        self.query_file_path = query_file_path\n",
    "        self.gateway_url = \"http://localhost:33000/api/gql\"\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def perform_queries(self, query, num_queries=100):\n",
    "        times = []\n",
    "        error = False\n",
    "\n",
    "        for _ in range(num_queries):\n",
    "            start_time = time.time()\n",
    "            response = requests.post(self.gateway_url, json=query, headers=self.headers)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times.append((end_time - start_time) * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                error = True\n",
    "                break\n",
    "\n",
    "        return times, error\n",
    "\n",
    "    def save_query_times(self, query, times, folder_path=\"./queries_times\"):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"{query}.txt\")\n",
    "\n",
    "        with open(file_path, mode='w') as file:\n",
    "            for time_value in times:\n",
    "                file.write(f\"{time_value}\\n\")\n",
    "\n",
    "    def send_queries(self, num_queries, save_file_path):\n",
    "        with open(self.query_file_path, 'r') as file:\n",
    "            graphql_queries = json.load(file)\n",
    "\n",
    "        for query, graphql_query in graphql_queries.items():\n",
    "            payload = {\"query\": graphql_query}\n",
    "            times, error = self.perform_queries(payload, num_queries)\n",
    "\n",
    "            print(\"************ \", query, \" ************\")\n",
    "            if error:\n",
    "                print(\"ERROR\")\n",
    "                pass\n",
    "            else:\n",
    "                times.sort()\n",
    "                numpy_times = np.asarray(times, dtype=np.float32)\n",
    "                mean_val = np.mean(numpy_times).item()\n",
    "                variance = np.var(numpy_times).item()\n",
    "                print(\"Mean: \", mean_val, \"ms\")\n",
    "                print(\"Variance: \", variance)\n",
    "\n",
    "                self.save_query_times(query, times, save_file_path)\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "from scipy.stats import anderson\n",
    "\n",
    "class StatisticsTest:\n",
    "    @staticmethod\n",
    "    def test_normality(data):\n",
    "        _, p_value = stats.shapiro(data)\n",
    "        return p_value\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def test_exponential(data):\n",
    "        loc_estimate = min(data)\n",
    "        scale_estimate = 1 / np.mean(data)\n",
    "        _, p_value = stats.kstest(data, 'expon', args=(loc_estimate, scale_estimate))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest\n",
    "from scipy.stats import anderson\n",
    "# Define StatisticsGenerator class\n",
    "class StatisticsGenerator:\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_statistics(file_path):\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            query_times = [float(line.strip()) for line in file]\n",
    "\n",
    "        mean_val = np.mean(query_times)\n",
    "        median_val = np.median(query_times)\n",
    "        max_val = np.max(query_times)\n",
    "        min_val = np.min(query_times)\n",
    "        variance = np.var(query_times)\n",
    "        std_dev = np.std(query_times)\n",
    "        percentile_90 = np.percentile(query_times, 90)\n",
    "        return mean_val, median_val, max_val, min_val, variance, std_dev, percentile_90\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_data_dict(folder_paths, statistic='mean'):\n",
    "        data_dict = {}\n",
    "\n",
    "        for folder_path in folder_paths:\n",
    "            folder_data = {}\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    mean, median,max_val, min_val, variance, std_dev, percentile_90 = StatisticsGenerator.calculate_statistics(file_path)\n",
    "\n",
    "                    if statistic == 'mean':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = mean\n",
    "                    elif statistic == 'median':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = median\n",
    "                    elif statistic == 'max':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = max_val\n",
    "                    elif statistic == 'min':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = min_val\n",
    "                    elif statistic == 'variance':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = variance\n",
    "                    elif statistic == 'standard_deviation':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = std_dev\n",
    "                    elif statistic == 'percentile_90':\n",
    "                        folder_data[os.path.splitext(file_name)[0]] = percentile_90\n",
    "                        \n",
    "            folder_name = os.path.basename(folder_path).replace('queries_times_', '')\n",
    "            data_dict[folder_name] = folder_data\n",
    "\n",
    "        # print(data_dict)\n",
    "        return data_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_comparison_chart(folder_paths, statistic='mean'):\n",
    "        data_dict = StatisticsGenerator.generate_data_dict(folder_paths, statistic)\n",
    "\n",
    "        # Flatten the dictionary to get a list of tuples (folder_name, file_name, stat_value)\n",
    "        flat_data = [(folder_name, file_name, stat_value) for folder_name, folder_data in data_dict.items() for file_name, stat_value in folder_data.items()]\n",
    "\n",
    "        # Sort the data alphabetically primarily by file_name and secondarily by folder_name\n",
    "        sorted_data = sorted(flat_data, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        labels = [f\"{file_name}\" for _, file_name, _ in sorted_data]\n",
    "        values = [stat_value for _, _, stat_value in sorted_data]\n",
    "        folder_names = [folder_name for folder_name, _, _ in sorted_data]\n",
    "\n",
    "        # Sort unique folder names alphabetically for consistency after regenerating\n",
    "        unique_folder_names = sorted(set(folder_names))\n",
    "\n",
    "        # Create a color map dictionary based on alphabetical order\n",
    "        color_map = {folder_name: plt.cm.tab10(i) for i, folder_name in enumerate(unique_folder_names)}\n",
    "        # Assign colors based on alphabetical order\n",
    "        colors = [color_map[folder_name] for folder_name in folder_names]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.3\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Adjust figure size as needed\n",
    "        bars = ax.bar(x, values, width, label=statistic.capitalize(), color=colors)\n",
    "\n",
    "        ax.set_ylabel(f'{statistic.capitalize()} Values')\n",
    "        ax.set_title(f'Comparison of {statistic.capitalize()}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "        # Manually create legend handles with appropriate colors\n",
    "        legend_handles = [plt.Line2D([0], [0], color=color_map[folder_name], lw=4) for folder_name in unique_folder_names]\n",
    "        legend_labels = [f'{folder}' for folder in unique_folder_names]\n",
    "\n",
    "        # Add legend with custom handles and labels\n",
    "        ax.legend(legend_handles, legend_labels)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.close()\n",
    "\n",
    "        return img_buf\n",
    "  \n",
    "    @staticmethod\n",
    "    def generate_basic_graph(data, filename):\n",
    "        # Function implementation\n",
    "        plt.figure()\n",
    "        plt.plot(data, marker='o', linestyle='-')\n",
    "        plt.title(f\"Basic Graph - {filename}\")\n",
    "        plt.xlabel(\"Query\")\n",
    "        plt.ylabel(\"Time (ms)\")\n",
    "        plt.grid(True)\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "    @staticmethod\n",
    "    def generate_histogram(data, filename):\n",
    "        plt.figure()\n",
    "        \n",
    "        # Square Root Rule\n",
    "        num_bins = int(np.sqrt(len(data)))\n",
    "\n",
    "        # Plot the histogram\n",
    "        plt.hist(data, bins=num_bins, color='blue', edgecolor='black', density=True, alpha=0.7, label='Data Histogram')\n",
    "\n",
    "        # Overlay normal distribution curve\n",
    "        mu, sigma = mean(data), stdev(data)\n",
    "        x = np.linspace(min(data), max(data), 100)\n",
    "        y = norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\n",
    "\n",
    "        # Overlay exponential distribution curve\n",
    "        loc, scale = min(data), stdev(data)  # Adjusted scale parameter\n",
    "        y_exponential = expon.pdf(x, loc, scale)\n",
    "        plt.plot(x, y_exponential, 'g-', linewidth=2, label='Exponential Distribution')\n",
    "        \n",
    "        plt.title(f\"Histogram - {filename}\")\n",
    "        plt.xlabel(\"Time (ms)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_stats_docx(folder_paths):\n",
    "        \n",
    "        overall_doc = docx.Document()\n",
    "        \n",
    "        # for stat_type in ['mean', 'variance', 'standard_deviation']:\n",
    "        for stat_type in ['mean', 'median', 'max', 'min', 'variance', 'standard_deviation', 'percentile_90']:\n",
    "\n",
    "            # Generate comparison chart and add it to the overall doc\n",
    "            img_buf = StatisticsGenerator.generate_comparison_chart(folder_paths, statistic=stat_type)\n",
    "            overall_doc.add_heading(f\"Comparison Chart - {stat_type.capitalize()}\", level=1)\n",
    "            overall_doc.add_picture(img_buf, width=docx.shared.Inches(7))\n",
    "            overall_doc.add_page_break()\n",
    "        \n",
    "        # overall_output_path = os.path.join(\"./\", \"overall_stats.docx\")\n",
    "        # overall_output_path = folder_paths.join(\"./\", \"overall_stats.docx\")\n",
    "        overall_output_path = os.path.join(folder_paths[0], \"overall_stats.docx\")\n",
    "        overall_doc.save(overall_output_path)\n",
    "\n",
    "        for folder_path in folder_paths:\n",
    "            doc = docx.Document()\n",
    "            doc.add_heading(f\"Statistics for Files in {folder_path}\", level=1)\n",
    "\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read().splitlines()\n",
    "\n",
    "                        # Extract numerical data from the file\n",
    "                        data = [float(line) for line in content if line.strip().replace('.', '').isdigit()]\n",
    "\n",
    "                        if data:\n",
    "                            # Basic statistics\n",
    "                            doc.add_heading(f\"File: {filename}\", level=2)\n",
    "                            doc.add_paragraph(f\"Number of queries: {round(len(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Mean: {round(np.mean(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Median: {round(np.median(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Max: {round(np.max(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Min: {round(np.min(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Variance: {round(np.var(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Standard Deviation: {round(np.std(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"90th Percentile: {round(np.percentile(data, 90), 2)}\")\n",
    "                            \n",
    "                            # Perform normality test\n",
    "                            p_value_normality = StatisticsTest.test_normality(data)\n",
    "                            doc.add_paragraph(f\"Is normal: {'Yes' if p_value_normality > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tShapiro-Wilk test p-value: {p_value_normality}\")\n",
    "\n",
    "                            # Perform exponential distribution test\n",
    "                            p_value_exponential = StatisticsTest.test_exponential(data)\n",
    "                            doc.add_paragraph(f\"Is exponential: {'Yes' if p_value_exponential > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tKolmogorov-Smirnov test p-value: {p_value_exponential}\")\n",
    "                            doc.add_paragraph(\"\\n\")\n",
    "\n",
    "                            #print out the paragraph\n",
    "                            \n",
    "\n",
    "                            # Generate basic graph\n",
    "                            img_buf_basic = StatisticsGenerator.generate_basic_graph(data, filename)\n",
    "                            doc.add_picture(img_buf_basic, width=docx.shared.Inches(6))\n",
    "\n",
    "                            # Generate histogram\n",
    "                            img_buf_histogram = StatisticsGenerator.generate_histogram(data, filename)\n",
    "                            doc.add_picture(img_buf_histogram, width=docx.shared.Inches(6))\n",
    "                            doc.add_page_break()\n",
    "\n",
    "            output_docx_path = os.path.join(folder_path, \"statistics.docx\")\n",
    "            doc.save(output_docx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nơi lấy các file query\n",
    "query_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Queries/gql_queries.json'\n",
    "num_queries = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cái này ản hưởng đến các file time txt\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/queries_times'\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Result'\n",
    "save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Result/cockroach'\n",
    "\n",
    "# Cái này ản hưởng đến overall_stats.docx và statistics.docx\n",
    "folder_paths = ['D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Result/cockroach']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************  gql_ug  ************\n",
      "Mean:  16.184181213378906 ms\n",
      "Variance:  61.410160064697266\n",
      "\n",
      "************  gql_facilities  ************\n",
      "ERROR\n",
      "\n",
      "************  gql_forms  ************\n",
      "Mean:  25.585447311401367 ms\n",
      "Variance:  145.6556854248047\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "analyzer = QueryPerformanceAnalyzer(query_file_path)\n",
    "analyzer.send_queries(num_queries, save_file_path)\n",
    "\n",
    "# StatisticsGenerator.generate_stats_docx(folder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_stats_docx(folder_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
