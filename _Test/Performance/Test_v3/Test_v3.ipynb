{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import docx\n",
    "import aiohttp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, expon\n",
    "from statistics import mean, stdev\n",
    "from io import BytesIO\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPerformanceAnalyzer:\n",
    "    def __init__(self, query_file_path):\n",
    "        self.query_file_path = query_file_path\n",
    "        self.gateway_url = \"http://localhost:33000/api/gql\"\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\\\n",
    "    \n",
    "    \n",
    "    def runSingleTestSet(self, query, num_queries):\n",
    "        times = []\n",
    "        error = False\n",
    "\n",
    "        for _ in range(num_queries):\n",
    "            start_time = time.time()\n",
    "            response = requests.post(self.gateway_url, json=query, headers=self.headers)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            print(response.text)\n",
    "            times.append((end_time - start_time) * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                error = True\n",
    "                break\n",
    "\n",
    "        return times, error\n",
    "\n",
    "\n",
    "\n",
    "    def save_times_results(self, query, times, folder_path):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"{query}.txt\")\n",
    "\n",
    "        with open(file_path, mode='w') as file:\n",
    "            for time_value in times:\n",
    "                file.write(f\"{time_value}\\n\")\n",
    "\n",
    "\n",
    "    def send_queries(self, num_queries, save_file_path):\n",
    "        with open(self.query_file_path, 'r') as file:\n",
    "            graphql_queries = json.load(file)\n",
    "\n",
    "        for query, graphql_query in graphql_queries.items():\n",
    "            # payload = {\"query\": graphql_query, \"variables\": {}}  # Variables are not needed for now\n",
    "            times, error = self.runSingleTestSet(graphql_query, num_queries)\n",
    "\n",
    "\n",
    "            print(\"************ \", query, \" ************\")\n",
    "            if error:\n",
    "                print(\"ERROR\")\n",
    "                pass\n",
    "            else:\n",
    "                times.sort()\n",
    "                numpy_times = np.asarray(times, dtype=np.float32)\n",
    "\n",
    "                mean_val = np.mean(numpy_times).item()\n",
    "                variance = np.var(numpy_times).item()\n",
    "                max_val = np.max(numpy_times).item()\n",
    "                min_val = np.min(numpy_times).item()\n",
    "                median_val = np.median(numpy_times).item()\n",
    "                stdev_val = np.std(numpy_times).item()\n",
    "                percentile_90 = np.percentile(numpy_times, 90).item()\n",
    "\n",
    "\n",
    "                print(\"Mean: \", mean_val, \"ms\")\n",
    "                # print(\"Variance: \", variance)\n",
    "                # print(\"Max: \", max_val, \"ms\")\n",
    "                # print(\"Min: \", min_val, \"ms\")\n",
    "                # print(\"Median: \", median_val, \"ms\")\n",
    "                # print(\"Standard Deviation: \", stdev_val)\n",
    "                # print(\"90th Percentile: \", percentile_90, \"ms\")\n",
    "\n",
    "                self.save_times_results(query, times, save_file_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "    def runStressTest(self, num_queries):\n",
    "        with open(self.query_file_path, 'r') as file:\n",
    "            graphql_queries = json.load(file)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for query, graphql_query in graphql_queries.items():\n",
    "                for _ in range(num_queries):\n",
    "                    futures.append(executor.submit(self.runSingleTestSet, {\"query\": graphql_query}, num_queries))\n",
    "\n",
    "            results = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                times, error = future.result()\n",
    "                if error:\n",
    "                    print(\"ERROR\")\n",
    "                else:\n",
    "                    results.append(times)\n",
    "\n",
    "        return results\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsTest:\n",
    "    @staticmethod\n",
    "    def test_normality(data):\n",
    "        _, p_value = stats.shapiro(data)\n",
    "        return p_value\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def test_exponential(data):\n",
    "        loc_estimate = min(data)\n",
    "        scale_estimate = 1 / np.mean(data)\n",
    "        _, p_value = stats.kstest(data, 'expon', args=(loc_estimate, scale_estimate))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_statistic(file_path):\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            query_times = [float(line.strip()) for line in file]\n",
    "\n",
    "        mean_val = np.mean(query_times)\n",
    "        median_val = np.median(query_times)\n",
    "        max_val = np.max(query_times)\n",
    "        min_val = np.min(query_times)\n",
    "        variance = np.var(query_times)\n",
    "        std_dev = np.std(query_times)\n",
    "        percentile_90 = np.percentile(query_times, 90)\n",
    "        return mean_val, median_val, max_val, min_val, variance, std_dev, percentile_90\n",
    "    \n",
    "    def generate_data_dict(folder_paths, statistic='mean'):\n",
    "        data_dict = {}\n",
    "\n",
    "        for folder_path in folder_paths:\n",
    "            folder_data = {}\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    stats = StatisticsCalculator.calculate_statistic(file_path)\n",
    "                    stats_dict = {\n",
    "                        'mean': stats[0],\n",
    "                        'median': stats[1],\n",
    "                        'max': stats[2],\n",
    "                        'min': stats[3],\n",
    "                        'variance': stats[4],\n",
    "                        'standard_deviation': stats[5],\n",
    "                        'percentile_90': stats[6]\n",
    "                    }\n",
    "\n",
    "                    folder_data[os.path.splitext(file_name)[0]] = stats_dict.get(statistic)\n",
    "\n",
    "            folder_name = os.path.basename(folder_path).replace('queries_times_', '')\n",
    "            data_dict[folder_name] = folder_data\n",
    "\n",
    "        return data_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StatisticsGenerator:\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_comparison_chart(folder_paths, statistic='mean'):\n",
    "        data_dict = StatisticsCalculator.generate_data_dict(folder_paths, statistic)\n",
    "\n",
    "        # Flatten the dictionary to get a list of tuples (folder_name, file_name, stat_value)\n",
    "        flat_data = [(folder_name, file_name, stat_value) for folder_name, folder_data in data_dict.items() for file_name, stat_value in folder_data.items()]\n",
    "\n",
    "        # Sort the data alphabetically primarily by file_name and secondarily by folder_name\n",
    "        sorted_data = sorted(flat_data, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        labels = [f\"{file_name}\" for _, file_name, _ in sorted_data]\n",
    "        values = [stat_value for _, _, stat_value in sorted_data]\n",
    "        folder_names = [folder_name for folder_name, _, _ in sorted_data]\n",
    "\n",
    "        # Sort unique folder names alphabetically for consistency after regenerating\n",
    "        unique_folder_names = sorted(set(folder_names))\n",
    "\n",
    "        # Create a color map dictionary based on alphabetical order\n",
    "        color_map = {folder_name: plt.cm.tab10(i) for i, folder_name in enumerate(unique_folder_names)}\n",
    "        # Assign colors based on alphabetical order\n",
    "        colors = [color_map[folder_name] for folder_name in folder_names]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.3\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Adjust figure size as needed\n",
    "        bars = ax.bar(x, values, width, label=statistic.capitalize(), color=colors)\n",
    "\n",
    "        ax.set_ylabel(f'{statistic.capitalize()} Values')\n",
    "        ax.set_title(f'Comparison of {statistic.capitalize()}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "        # Manually create legend handles with appropriate colors\n",
    "        legend_handles = [plt.Line2D([0], [0], color=color_map[folder_name], lw=4) for folder_name in unique_folder_names]\n",
    "        legend_labels = [f'{folder}' for folder in unique_folder_names]\n",
    "\n",
    "        # Add legend with custom handles and labels\n",
    "        ax.legend(legend_handles, legend_labels)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.close()\n",
    "\n",
    "        return img_buf\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_basic_graph(data, filename):\n",
    "        # Function implementation\n",
    "        plt.figure()\n",
    "        plt.plot(data, marker='o', linestyle='-')\n",
    "        plt.title(f\"Basic Graph - {filename}\")\n",
    "        plt.xlabel(\"Query\")\n",
    "        plt.ylabel(\"Time (ms)\")\n",
    "        plt.grid(True)\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_histogram(data, filename):\n",
    "        plt.figure()\n",
    "        \n",
    "        # Square Root Rule\n",
    "        num_bins = int(np.sqrt(len(data)))\n",
    "\n",
    "        # Plot the histogram\n",
    "        plt.hist(data, bins=num_bins, color='blue', edgecolor='black', density=True, alpha=0.7, label='Data Histogram')\n",
    "\n",
    "        # Overlay normal distribution curve\n",
    "        mu, sigma = mean(data), stdev(data)\n",
    "        x = np.linspace(min(data), max(data), 100)\n",
    "        y = norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\n",
    "\n",
    "        # Overlay exponential distribution curve\n",
    "        loc, scale = min(data), stdev(data)  # Adjusted scale parameter\n",
    "        y_exponential = expon.pdf(x, loc, scale)\n",
    "        plt.plot(x, y_exponential, 'g-', linewidth=2, label='Exponential Distribution')\n",
    "        \n",
    "        plt.title(f\"Histogram - {filename}\")\n",
    "        plt.xlabel(\"Time (ms)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_overall_stats(folder_paths):\n",
    "        \n",
    "        overall_doc = docx.Document()\n",
    "        \n",
    "        # for stat_type in ['mean', 'variance', 'standard_deviation']:\n",
    "        for stat_type in ['mean', 'median', 'max', 'min', 'variance', 'standard_deviation', 'percentile_90']:\n",
    "\n",
    "            # Generate comparison chart and add it to the overall doc\n",
    "            img_buf = StatisticsGenerator.generate_comparison_chart(folder_paths, statistic=stat_type)\n",
    "            overall_doc.add_heading(f\"Comparison Chart - {stat_type.capitalize()}\", level=1)\n",
    "            overall_doc.add_picture(img_buf, width=docx.shared.Inches(7))\n",
    "            overall_doc.add_page_break()\n",
    "        \n",
    "        # overall_output_path = os.path.join(\"./\", \"overall_stats.docx\")\n",
    "        # overall_output_path = folder_paths.join(\"./\", \"overall_stats.docx\")\n",
    "        overall_output_path = os.path.join(folder_paths[0], \"overall_stats.docx\")\n",
    "        overall_doc.save(overall_output_path)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_stats_docx(folder_paths):\n",
    "        for folder_path in folder_paths:\n",
    "            doc = docx.Document()\n",
    "            doc.add_heading(f\"Statistics for Files in {folder_path}\", level=1)\n",
    "\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read().splitlines()\n",
    "\n",
    "                        # Extract numerical data from the file\n",
    "                        data = [float(line) for line in content if line.strip().replace('.', '').isdigit()]\n",
    "\n",
    "                        if data:\n",
    "                            # Basic statistics\n",
    "                            doc.add_heading(f\"File: {filename}\", level=2)\n",
    "                            doc.add_paragraph(f\"Number of queries: {round(len(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Mean: {round(np.mean(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Median: {round(np.median(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Max: {round(np.max(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Min: {round(np.min(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Variance: {round(np.var(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Standard Deviation: {round(np.std(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"90th Percentile: {round(np.percentile(data, 90), 2)}\")\n",
    "                            \n",
    "                            # Perform normality test\n",
    "                            p_value_normality = StatisticsTest.test_normality(data)\n",
    "                            doc.add_paragraph(f\"Is normal: {'Yes' if p_value_normality > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tShapiro-Wilk test p-value: {p_value_normality}\")\n",
    "\n",
    "                            # Perform exponential distribution test\n",
    "                            p_value_exponential = StatisticsTest.test_exponential(data)\n",
    "                            doc.add_paragraph(f\"Is exponential: {'Yes' if p_value_exponential > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tKolmogorov-Smirnov test p-value: {p_value_exponential}\")\n",
    "                            doc.add_paragraph(\"\\n\")\n",
    "\n",
    "                            #print out the paragraph\n",
    "                            \n",
    "\n",
    "                            # Generate basic graph\n",
    "                            img_buf_basic = StatisticsGenerator.generate_basic_graph(data, filename)\n",
    "                            doc.add_picture(img_buf_basic, width=docx.shared.Inches(6))\n",
    "\n",
    "                            # Generate histogram\n",
    "                            img_buf_histogram = StatisticsGenerator.generate_histogram(data, filename)\n",
    "                            doc.add_picture(img_buf_histogram, width=docx.shared.Inches(6))\n",
    "                            doc.add_page_break()\n",
    "\n",
    "            output_docx_path = os.path.join(folder_path, \"statistics.docx\")\n",
    "            doc.save(output_docx_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"groupById\":{\"id\":\"2d9dcd22-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Uni\",\"roles\":[{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\",\"name\":\"John\",\"surname\":\"Newbie\",\"email\":\"john.newbie@world.com\"},\"roletype\":{\"id\":\"ced46aa4-3217-4fc1-b79d-f6be7d21c6b6\",\"name\":\"administrátor\"}},{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\",\"name\":\"John\",\"surname\":\"Newbie\",\"email\":\"john.newbie@world.com\"},\"roletype\":{\"id\":\"ae3f0d74-6159-11ed-b753-0242ac120003\",\"name\":\"rektor\"}}],\"subgroups\":[{\"id\":\"2d9dced0-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Fac\"}],\"memberships\":[{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\",\"name\":\"John\"}},{\"user\":{\"id\":\"2d9dc868-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Julia\"}},{\"user\":{\"id\":\"2d9dc9a8-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Johnson\"}},{\"user\":{\"id\":\"2d9dcbec-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Jepeto\"}},{\"user\":{\"id\":\"89d1e724-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jana\"}},{\"user\":{\"id\":\"89d1f34a-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jolana\"}},{\"user\":{\"id\":\"89d1f3cc-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jitka\"}},{\"user\":{\"id\":\"89d1f430-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jaroslava\"}},{\"user\":{\"id\":\"89d1f48a-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Lada\"}},{\"user\":{\"id\":\"89d1f4e4-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Ludmila\"}},{\"user\":{\"id\":\"89d1f534-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Lucie\"}},{\"user\":{\"id\":\"89d1f58e-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Nola\"}},{\"user\":{\"id\":\"89d1f5de-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Neva\"}},{\"user\":{\"id\":\"89d1f638-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Nora\"}}]}}}\n",
      "\n",
      "{\"data\":{\"groupById\":{\"id\":\"2d9dcd22-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Uni\",\"roles\":[{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\",\"name\":\"John\",\"surname\":\"Newbie\",\"email\":\"john.newbie@world.com\"},\"roletype\":{\"id\":\"ced46aa4-3217-4fc1-b79d-f6be7d21c6b6\",\"name\":\"administrátor\"}},{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\",\"name\":\"John\",\"surname\":\"Newbie\",\"email\":\"john.newbie@world.com\"},\"roletype\":{\"id\":\"ae3f0d74-6159-11ed-b753-0242ac120003\",\"name\":\"rektor\"}}],\"subgroups\":[{\"id\":\"2d9dced0-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Fac\"}],\"memberships\":[{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\",\"name\":\"John\"}},{\"user\":{\"id\":\"2d9dc868-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Julia\"}},{\"user\":{\"id\":\"2d9dc9a8-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Johnson\"}},{\"user\":{\"id\":\"2d9dcbec-a4a2-11ed-b9df-0242ac120003\",\"name\":\"Jepeto\"}},{\"user\":{\"id\":\"89d1e724-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jana\"}},{\"user\":{\"id\":\"89d1f34a-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jolana\"}},{\"user\":{\"id\":\"89d1f3cc-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jitka\"}},{\"user\":{\"id\":\"89d1f430-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Jaroslava\"}},{\"user\":{\"id\":\"89d1f48a-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Lada\"}},{\"user\":{\"id\":\"89d1f4e4-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Ludmila\"}},{\"user\":{\"id\":\"89d1f534-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Lucie\"}},{\"user\":{\"id\":\"89d1f58e-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Nola\"}},{\"user\":{\"id\":\"89d1f5de-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Neva\"}},{\"user\":{\"id\":\"89d1f638-ae0f-11ed-9bd8-0242ac110002\",\"name\":\"Nora\"}}]}}}\n",
      "\n",
      "************  gql_ug_test_large_query  ************\n",
      "Mean:  62.67869567871094 ms\n",
      "\n",
      "{\"data\":{\"roleTypePage\":[{\"id\":\"05a3e0f5-f71e-4caa-8012-229d868aa8ca\",\"name\":\"já\"},{\"id\":\"ced46aa4-3217-4fc1-b79d-f6be7d21c6b6\",\"name\":\"administrátor\"},{\"id\":\"b87aed46-dfc3-40f8-ad49-03f4138c7478\",\"name\":\"zpracovatel gdpr\"},{\"id\":\"ae3f0d74-6159-11ed-b753-0242ac120003\",\"name\":\"rektor\"},{\"id\":\"ae3f2886-6159-11ed-b753-0242ac120003\",\"name\":\"prorektor\"},{\"id\":\"ae3f2912-6159-11ed-b753-0242ac120003\",\"name\":\"děkan\"},{\"id\":\"ae3f2980-6159-11ed-b753-0242ac120003\",\"name\":\"proděkan\"},{\"id\":\"ae3f29ee-6159-11ed-b753-0242ac120003\",\"name\":\"vedoucí katedry\"},{\"id\":\"ae3f2a5c-6159-11ed-b753-0242ac120003\",\"name\":\"vedoucí učitel\"},{\"id\":\"5f0c247e-931f-11ed-9b95-0242ac110002\",\"name\":\"garant\"}]}}\n",
      "\n",
      "{\"data\":{\"roleTypePage\":[{\"id\":\"05a3e0f5-f71e-4caa-8012-229d868aa8ca\",\"name\":\"já\"},{\"id\":\"ced46aa4-3217-4fc1-b79d-f6be7d21c6b6\",\"name\":\"administrátor\"},{\"id\":\"b87aed46-dfc3-40f8-ad49-03f4138c7478\",\"name\":\"zpracovatel gdpr\"},{\"id\":\"ae3f0d74-6159-11ed-b753-0242ac120003\",\"name\":\"rektor\"},{\"id\":\"ae3f2886-6159-11ed-b753-0242ac120003\",\"name\":\"prorektor\"},{\"id\":\"ae3f2912-6159-11ed-b753-0242ac120003\",\"name\":\"děkan\"},{\"id\":\"ae3f2980-6159-11ed-b753-0242ac120003\",\"name\":\"proděkan\"},{\"id\":\"ae3f29ee-6159-11ed-b753-0242ac120003\",\"name\":\"vedoucí katedry\"},{\"id\":\"ae3f2a5c-6159-11ed-b753-0242ac120003\",\"name\":\"vedoucí učitel\"},{\"id\":\"5f0c247e-931f-11ed-9b95-0242ac110002\",\"name\":\"garant\"}]}}\n",
      "\n",
      "************  gql_roletypes  ************\n",
      "Mean:  26.89754867553711 ms\n",
      "\n",
      "{\"data\":{\"groupById\":{\"memberships\":[{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"2d9dc868-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"2d9dc9a8-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"2d9dcbec-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"89d1e724-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f34a-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f3cc-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f430-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f48a-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f4e4-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f534-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f58e-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f5de-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f638-ae0f-11ed-9bd8-0242ac110002\"}}],\"id\":\"2d9dcd22-a4a2-11ed-b9df-0242ac120003\"}}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"groupById\":{\"memberships\":[{\"user\":{\"id\":\"2d9dc5ca-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"2d9dc868-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"2d9dc9a8-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"2d9dcbec-a4a2-11ed-b9df-0242ac120003\"}},{\"user\":{\"id\":\"89d1e724-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f34a-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f3cc-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f430-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f48a-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f4e4-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f534-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f58e-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f5de-ae0f-11ed-9bd8-0242ac110002\"}},{\"user\":{\"id\":\"89d1f638-ae0f-11ed-9bd8-0242ac110002\"}}],\"id\":\"2d9dcd22-a4a2-11ed-b9df-0242ac120003\"}}}\n",
      "\n",
      "************  gql_ug_v2  ************\n",
      "Mean:  57.99842071533203 ms\n",
      "\n",
      "{\"data\":{\"userInsert\":{\"id\":\"5b934b5b-0c4c-48d4-9341-eb284bafa955\",\"msg\":\"ok\",\"user\":{\"id\":\"5b934b5b-0c4c-48d4-9341-eb284bafa955\",\"name\":\"New User\",\"lastchange\":\"2024-04-19T00:50:08.884625\"}}}}\n",
      "\n",
      "{\"data\":{\"userInsert\":{\"id\":\"36c44707-6fcf-42d8-a026-5d92e166abf5\",\"msg\":\"ok\",\"user\":{\"id\":\"36c44707-6fcf-42d8-a026-5d92e166abf5\",\"name\":\"New User\",\"lastchange\":\"2024-04-19T00:50:08.953037\"}}}}\n",
      "\n",
      "************  user_insert  ************\n",
      "Mean:  68.10855865478516 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Queries/gql_queries_3_v2.json'\n",
    "save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres'\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v2/yugabyte'\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v2/cockroach'\n",
    "\n",
    "\n",
    "query_performance_analyzer = QueryPerformanceAnalyzer(query_file_path)\n",
    "query_performance_analyzer.send_queries(2, save_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_overall_stats([save_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be at least length 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-33d5cf1c28bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mStatisticsGenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_stats_docx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_file_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-206-36ea9a8447b9>\u001b[0m in \u001b[0;36mgenerate_stats_docx\u001b[1;34m(folder_paths)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                             \u001b[1;31m# Perform normality test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                             \u001b[0mp_value_normality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStatisticsTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_normality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m                             \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_paragraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Is normal: {'Yes' if p_value_normality > 0.05 else 'No'}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                             \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_paragraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\tShapiro-Wilk test p-value: {p_value_normality}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-204-ab16da41d3a4>\u001b[0m in \u001b[0;36mtest_normality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_normality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshapiro\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mp_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\dangq\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\stats\\_morestats.py\u001b[0m in \u001b[0;36mshapiro\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1867\u001b[0m     \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1868\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1869\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Data must be at least length 3.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1871\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data must be at least length 3."
     ]
    }
   ],
   "source": [
    "StatisticsGenerator.generate_stats_docx([save_file_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
