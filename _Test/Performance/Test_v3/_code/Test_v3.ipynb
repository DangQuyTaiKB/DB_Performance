{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import docx\n",
    "import aiohttp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, expon\n",
    "from statistics import mean, stdev\n",
    "from io import BytesIO\n",
    "import aiofiles\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import uuid\n",
    "\n",
    "class QueryPerformanceAnalyzer:\n",
    "  def __init__(self, query_file_path):\n",
    "    self.query_file_path = query_file_path\n",
    "    self.gateway_url = \"http://localhost:33000/api/gql\"\n",
    "    self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "  def randomUUID(self):\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "  async def runSingleModeUserTest(self, query, num_queries):\n",
    "    times = []\n",
    "    error = False\n",
    "\n",
    "    for _ in range(num_queries):\n",
    "      print(\"query: \", query)\n",
    "      if \"userInsert\" in query[\"query\"]:\n",
    "        query[\"variables\"][\"id\"] = self.randomUUID()\n",
    "        query[\"variables\"][\"name\"] = \"user\" + str(_)\n",
    "        query[\"variables\"][\"surname\"] = \"surname\" + str(_)\n",
    "        query[\"variables\"][\"email\"] = \"user\" + str(_) + \"@gmail.com\"\n",
    "        print(\"payload: \", query)\n",
    "      if \"userUpdate\" in query[\"query\"]:\n",
    "        query[\"variables\"][\"name\"] = \"newname\" + str(_)\n",
    "        # lastchange = now \n",
    "        query[\"variables\"][\"lastchange\"] = datetime.now().isoformat()\n",
    "        print(\"payload: \", query)\n",
    "\n",
    "\n",
    "      async with aiohttp.ClientSession() as session:\n",
    "        start_time = time.time()\n",
    "        async with session.post(self.gateway_url, json=query, headers=self.headers) as response:\n",
    "          end_time = time.time()\n",
    "          if response.status != 200:\n",
    "            error = True\n",
    "            break\n",
    "        times.append((end_time - start_time) * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "    return times, error\n",
    "\n",
    "  async def save_times_results(self, query_times, save_file_path):\n",
    "    os.makedirs(save_file_path, exist_ok=True)\n",
    "\n",
    "    for query, times in query_times.items():\n",
    "      file_path = os.path.join(save_file_path, f\"{query}.txt\")\n",
    "      async with aiofiles.open(file_path, mode='w') as file:\n",
    "        for time_value in times:\n",
    "          await file.write(f\"{time_value}\\n\")\n",
    "\n",
    "  async def load_queries(self):\n",
    "    async with aiofiles.open(self.query_file_path, mode='r') as file:\n",
    "      return json.loads(await file.read())\n",
    "    \n",
    "\n",
    "\n",
    "  async def send_queries(self, num_queries, save_file_path):\n",
    "    graphql_queries = await self.load_queries()  # Asynchronously load queries from file\n",
    "    \n",
    "    tasks = []\n",
    "    query_times = {}  # Dictionary to store times for each query\n",
    "\n",
    "    # if the query is insert, we need to change the variables to be unique for each query and change the payload \n",
    "    \n",
    "    for query, graphql_query in graphql_queries.items():\n",
    "      # if query == \"user_insert\":\n",
    "      #   payload = {\n",
    "      #     \"query\": graphql_query[\"query\"],\n",
    "      #     \"variables\": {\n",
    "      #       \"id\": self.randomUUID(),\n",
    "      #       \"name\": \"user\" + self.randomUUID(),\n",
    "      #       \"email\": \"user\" + self.randomUUID() + \"@gmail.com\"}\n",
    "      #   }\n",
    "      # else:\n",
    "      #   payload = {\n",
    "      #       \"query\": graphql_query[\"query\"],\n",
    "      #       \"variables\": graphql_query[\"variables\"]}\n",
    "      payload = {\n",
    "          \"query\": graphql_query[\"query\"],\n",
    "          \"variables\": graphql_query[\"variables\"]}\n",
    "\n",
    "      print(\"payload: \", payload)\n",
    "      tasks.append(asyncio.create_task(self.runSingleModeUserTest(payload, num_queries)))\n",
    "\n",
    "    results = await asyncio.gather(*tasks)  # Gather results from all tasks\n",
    "\n",
    "    for query, (times, error) in zip(graphql_queries.keys(), results):\n",
    "      if error:\n",
    "        print(\"ERROR for query:\", query)\n",
    "      else:\n",
    "        query_times[query] = times\n",
    "        times.sort()\n",
    "        numpy_times = np.asarray(times, dtype=np.float32)\n",
    "\n",
    "        # Calculate and print statistics for the current query\n",
    "        mean_val = np.mean(numpy_times).item()\n",
    "        median_val = np.median(numpy_times).item()\n",
    "        max_val = np.max(numpy_times).item()\n",
    "        min_val = np.min(numpy_times).item()\n",
    "        variance = np.var(numpy_times).item()\n",
    "        stdev_val = np.std(numpy_times).item()\n",
    "        percentile_90 = np.percentile(numpy_times, 90).item()\n",
    "\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"Mean: \", mean_val, \"ms\")\n",
    "        # print(\"Median: \", median_val, \"ms\")\n",
    "        # print(\"Max: \", max_val, \"ms\")\n",
    "        # print(\"Min: \", min_val, \"ms\")\n",
    "        # print(\"Variance: \", variance, \"ms^2\")\n",
    "        # print(\"Std Dev: \", stdev_val, \"ms\")\n",
    "        # print(\"90th Percentile: \", percentile_90, \"ms\")\n",
    "        # print()  # Print a newline between queries\n",
    "\n",
    "    await self.save_times_results(query_times, save_file_path)  # Save times for all queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsTest:\n",
    "    @staticmethod\n",
    "    def test_normality(data):\n",
    "        _, p_value = stats.shapiro(data)\n",
    "        return p_value\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def test_exponential(data):\n",
    "        loc_estimate = min(data)\n",
    "        scale_estimate = 1 / np.mean(data)\n",
    "        _, p_value = stats.kstest(data, 'expon', args=(loc_estimate, scale_estimate))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_statistic(file_path):\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            query_times = [float(line.strip()) for line in file]\n",
    "\n",
    "        mean_val = np.mean(query_times)\n",
    "        median_val = np.median(query_times)\n",
    "        max_val = np.max(query_times)\n",
    "        min_val = np.min(query_times)\n",
    "        variance = np.var(query_times)\n",
    "        std_dev = np.std(query_times)\n",
    "        percentile_90 = np.percentile(query_times, 90)\n",
    "        return mean_val, median_val, max_val, min_val, variance, std_dev, percentile_90\n",
    "    \n",
    "    def generate_data_dict(folder_paths, statistic='mean'):\n",
    "        data_dict = {}\n",
    "\n",
    "        for folder_path in folder_paths:\n",
    "            folder_data = {}\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    stats = StatisticsCalculator.calculate_statistic(file_path)\n",
    "                    stats_dict = {\n",
    "                        'mean': stats[0],\n",
    "                        'median': stats[1],\n",
    "                        'max': stats[2],\n",
    "                        'min': stats[3],\n",
    "                        'variance': stats[4],\n",
    "                        'standard_deviation': stats[5],\n",
    "                        'percentile_90': stats[6]\n",
    "                    }\n",
    "\n",
    "                    folder_data[os.path.splitext(file_name)[0]] = stats_dict.get(statistic)\n",
    "\n",
    "            folder_name = os.path.basename(folder_path).replace('queries_times_', '')\n",
    "            data_dict[folder_name] = folder_data\n",
    "\n",
    "        return data_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StatisticsGenerator:\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_comparison_chart(folder_paths, statistic='mean'):\n",
    "        data_dict = StatisticsCalculator.generate_data_dict(folder_paths, statistic)\n",
    "\n",
    "        # Flatten the dictionary to get a list of tuples (folder_name, file_name, stat_value)\n",
    "        flat_data = [(folder_name, file_name, stat_value) for folder_name, folder_data in data_dict.items() for file_name, stat_value in folder_data.items()]\n",
    "\n",
    "        # Sort the data alphabetically primarily by file_name and secondarily by folder_name\n",
    "        sorted_data = sorted(flat_data, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        labels = [f\"{file_name}\" for _, file_name, _ in sorted_data]\n",
    "        values = [stat_value for _, _, stat_value in sorted_data]\n",
    "        folder_names = [folder_name for folder_name, _, _ in sorted_data]\n",
    "\n",
    "        # Sort unique folder names alphabetically for consistency after regenerating\n",
    "        unique_folder_names = sorted(set(folder_names))\n",
    "\n",
    "        # Create a color map dictionary based on alphabetical order\n",
    "        color_map = {folder_name: plt.cm.tab10(i) for i, folder_name in enumerate(unique_folder_names)}\n",
    "        # Assign colors based on alphabetical order\n",
    "        colors = [color_map[folder_name] for folder_name in folder_names]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.3\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Adjust figure size as needed\n",
    "        bars = ax.bar(x, values, width, label=statistic.capitalize(), color=colors)\n",
    "\n",
    "        ax.set_ylabel(f'{statistic.capitalize()} Values')\n",
    "        ax.set_title(f'Comparison of {statistic.capitalize()}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "        # Manually create legend handles with appropriate colors\n",
    "        legend_handles = [plt.Line2D([0], [0], color=color_map[folder_name], lw=4) for folder_name in unique_folder_names]\n",
    "        legend_labels = [f'{folder}' for folder in unique_folder_names]\n",
    "\n",
    "        # Add legend with custom handles and labels\n",
    "        ax.legend(legend_handles, legend_labels)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.close()\n",
    "\n",
    "        return img_buf\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_basic_graph(data, filename):\n",
    "        # Function implementation\n",
    "        plt.figure()\n",
    "        plt.plot(data, marker='o', linestyle='-')\n",
    "        plt.title(f\"Basic Graph - {filename}\")\n",
    "        plt.xlabel(\"Query\")\n",
    "        plt.ylabel(\"Time (ms)\")\n",
    "        plt.grid(True)\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_histogram(data, filename):\n",
    "        plt.figure()\n",
    "        \n",
    "        # Square Root Rule\n",
    "        num_bins = int(np.sqrt(len(data)))\n",
    "\n",
    "        # Plot the histogram\n",
    "        plt.hist(data, bins=num_bins, color='blue', edgecolor='black', density=True, alpha=0.7, label='Data Histogram')\n",
    "\n",
    "        # Overlay normal distribution curve\n",
    "        mu, sigma = mean(data), stdev(data)\n",
    "        x = np.linspace(min(data), max(data), 100)\n",
    "        y = norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\n",
    "\n",
    "        # Overlay exponential distribution curve\n",
    "        loc, scale = min(data), stdev(data)  # Adjusted scale parameter\n",
    "        y_exponential = expon.pdf(x, loc, scale)\n",
    "        plt.plot(x, y_exponential, 'g-', linewidth=2, label='Exponential Distribution')\n",
    "        \n",
    "        plt.title(f\"Histogram - {filename}\")\n",
    "        plt.xlabel(\"Time (ms)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_overall_stats(folder_paths):\n",
    "        \n",
    "        overall_doc = docx.Document()\n",
    "        \n",
    "        # for stat_type in ['mean', 'variance', 'standard_deviation']:\n",
    "        for stat_type in ['mean', 'median', 'max', 'min', 'variance', 'standard_deviation', 'percentile_90']:\n",
    "\n",
    "            # Generate comparison chart and add it to the overall doc\n",
    "            img_buf = StatisticsGenerator.generate_comparison_chart(folder_paths, statistic=stat_type)\n",
    "            overall_doc.add_heading(f\"Comparison Chart - {stat_type.capitalize()}\", level=1)\n",
    "            overall_doc.add_picture(img_buf, width=docx.shared.Inches(7))\n",
    "            overall_doc.add_page_break()\n",
    "        \n",
    "        # overall_output_path = os.path.join(\"./\", \"overall_stats.docx\")\n",
    "        # overall_output_path = folder_paths.join(\"./\", \"overall_stats.docx\")\n",
    "        overall_output_path = os.path.join(folder_paths[0], \"overall_stats.docx\")\n",
    "        overall_doc.save(overall_output_path)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_stats_docx(folder_paths):\n",
    "        for folder_path in folder_paths:\n",
    "            doc = docx.Document()\n",
    "            doc.add_heading(f\"Statistics for Files in {folder_path}\", level=1)\n",
    "\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read().splitlines()\n",
    "\n",
    "                        # Extract numerical data from the file\n",
    "                        data = [float(line) for line in content if line.strip().replace('.', '').isdigit()]\n",
    "\n",
    "                        if data:\n",
    "                            # Basic statistics\n",
    "                            doc.add_heading(f\"File: {filename}\", level=2)\n",
    "                            doc.add_paragraph(f\"Number of queries: {round(len(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Mean: {round(np.mean(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Median: {round(np.median(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Max: {round(np.max(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Min: {round(np.min(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Variance: {round(np.var(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Standard Deviation: {round(np.std(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"90th Percentile: {round(np.percentile(data, 90), 2)}\")\n",
    "                            \n",
    "                            # Perform normality test\n",
    "                            p_value_normality = StatisticsTest.test_normality(data)\n",
    "                            doc.add_paragraph(f\"Is normal: {'Yes' if p_value_normality > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tShapiro-Wilk test p-value: {p_value_normality}\")\n",
    "\n",
    "                            # Perform exponential distribution test\n",
    "                            p_value_exponential = StatisticsTest.test_exponential(data)\n",
    "                            doc.add_paragraph(f\"Is exponential: {'Yes' if p_value_exponential > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tKolmogorov-Smirnov test p-value: {p_value_exponential}\")\n",
    "                            doc.add_paragraph(\"\\n\")\n",
    "\n",
    "                            #print out the paragraph\n",
    "                            \n",
    "\n",
    "                            # Generate basic graph\n",
    "                            img_buf_basic = StatisticsGenerator.generate_basic_graph(data, filename)\n",
    "                            doc.add_picture(img_buf_basic, width=docx.shared.Inches(6))\n",
    "\n",
    "                            # Generate histogram\n",
    "                            img_buf_histogram = StatisticsGenerator.generate_histogram(data, filename)\n",
    "                            doc.add_picture(img_buf_histogram, width=docx.shared.Inches(6))\n",
    "                            doc.add_page_break()\n",
    "\n",
    "            output_docx_path = os.path.join(folder_path, \"statistics.docx\")\n",
    "            doc.save(output_docx_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    query_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Queries/gql_queries_3_v2.json'\n",
    "    # save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres'\n",
    "    # save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/cockroach'\n",
    "    # save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/yugabyte'\n",
    "    \n",
    "    \n",
    "    # save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres_ha'\n",
    "    save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres_replica'\n",
    "\n",
    "    # query_performance_analyzer = QueryPerformanceAnalyzer(query_file_path)\n",
    "    # await query_performance_analyzer.send_queries(100, save_file_path)\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n",
    "# loop.close()\n",
    "await main()\n",
    "\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres'\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/cockroach'\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/yugabyte'\n",
    "\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres_ha'\n",
    "save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v3/postgres_replica'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_overall_stats([save_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_stats_docx([save_file_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
