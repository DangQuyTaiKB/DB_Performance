{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import docx\n",
    "import aiohttp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, expon\n",
    "from statistics import mean, stdev\n",
    "from io import BytesIO\n",
    "import aiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class QueryPerformanceAnalyzer:\n",
    "  def __init__(self, query_file_path):\n",
    "    self.query_file_path = query_file_path\n",
    "    self.gateway_url = \"http://localhost:33000/api/gql\"\n",
    "    self.headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "  async def runTest(self, query):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(self.gateway_url, json=query, headers=self.headers) as response:\n",
    "            return response\n",
    "\n",
    "  async def runSingleModeUserTest(self, query, num_queries):\n",
    "    times = []\n",
    "    error = False\n",
    "    awaitables = []\n",
    "    for _ in range(num_queries):\n",
    "      awaitables.append(self.runTest(query))\n",
    "\n",
    "    start_time = time.time()\n",
    "    responses = await asyncio.gather(*awaitables)\n",
    "    end_time = time.time()\n",
    "    times.append((end_time - start_time) * 1000)  # Convert seconds to milliseconds\n",
    "        # Check the status of each response\n",
    "    for response in responses:\n",
    "        if response.status != 200:\n",
    "            error = True\n",
    "            break\n",
    "    return times, error\n",
    "\n",
    "\n",
    "  async def save_times_results(self, query_times, save_file_path):\n",
    "    os.makedirs(save_file_path, exist_ok=True)\n",
    "\n",
    "    for query, (times, error)  in query_times.items():\n",
    "      file_path = os.path.join(save_file_path, f\"{query}.txt\")\n",
    "      async with aiofiles.open(file_path, mode='w') as file:\n",
    "        for time_value in times:\n",
    "          if time_value is None:\n",
    "            continue\n",
    "          await file.write(f\"{time_value}\\n\")\n",
    "\n",
    "  async def load_queries(self):\n",
    "    async with aiofiles.open(self.query_file_path, mode='r') as file:\n",
    "      return json.loads(await file.read())\n",
    "\n",
    "  async def send_queries(self, num_queries, save_file_path):\n",
    "    graphql_queries = await self.load_queries()  # Asynchronously load queries from file\n",
    "    \n",
    "    tasks = []\n",
    "    query_times = {}  # Dictionary to store times for each query\n",
    "    \n",
    "    for query, graphql_query in graphql_queries.items():\n",
    "      payload = {\"query\": graphql_query}\n",
    "      # i want to measure 100 time  for each query here\n",
    "\n",
    "      measurment_result= await self.runSingleModeUserTest(payload, num_queries)\n",
    "      print(measurment_result)\n",
    "      query_times[query] = measurment_result\n",
    "\n",
    "\n",
    "    for query, (times, error) in query_times.items():\n",
    "      if error:\n",
    "        print(\"ERROR for query:\", query)\n",
    "      else:\n",
    "        times.sort()\n",
    "        numpy_times = np.asarray(times, dtype=np.float32)\n",
    "\n",
    "        # Calculate and print statistics for the current query\n",
    "        mean_val = np.mean(numpy_times).item()\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"Mean: \", mean_val, \"ms\")\n",
    "\n",
    "    await self.save_times_results(query_times, save_file_path)  # Save times for all queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsTest:\n",
    "    @staticmethod\n",
    "    def test_normality(data):\n",
    "        _, p_value = stats.shapiro(data)\n",
    "        return p_value\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def test_exponential(data):\n",
    "        loc_estimate = min(data)\n",
    "        scale_estimate = 1 / np.mean(data)\n",
    "        _, p_value = stats.kstest(data, 'expon', args=(loc_estimate, scale_estimate))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_statistic(file_path):\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            query_times = [float(line.strip()) for line in file]\n",
    "\n",
    "        mean_val = np.mean(query_times)\n",
    "        median_val = np.median(query_times)\n",
    "        max_val = np.max(query_times)\n",
    "        min_val = np.min(query_times)\n",
    "        variance = np.var(query_times)\n",
    "        std_dev = np.std(query_times)\n",
    "        percentile_90 = np.percentile(query_times, 90)\n",
    "        return mean_val, median_val, max_val, min_val, variance, std_dev, percentile_90\n",
    "    \n",
    "    def generate_data_dict(folder_paths, statistic='mean'):\n",
    "        data_dict = {}\n",
    "\n",
    "        for folder_path in folder_paths:\n",
    "            folder_data = {}\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    stats = StatisticsCalculator.calculate_statistic(file_path)\n",
    "                    stats_dict = {\n",
    "                        'mean': stats[0],\n",
    "                        'median': stats[1],\n",
    "                        'max': stats[2],\n",
    "                        'min': stats[3],\n",
    "                        'variance': stats[4],\n",
    "                        'standard_deviation': stats[5],\n",
    "                        'percentile_90': stats[6]\n",
    "                    }\n",
    "\n",
    "                    folder_data[os.path.splitext(file_name)[0]] = stats_dict.get(statistic)\n",
    "\n",
    "            folder_name = os.path.basename(folder_path).replace('queries_times_', '')\n",
    "            data_dict[folder_name] = folder_data\n",
    "\n",
    "        return data_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StatisticsGenerator:\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_comparison_chart(folder_paths, statistic='mean'):\n",
    "        data_dict = StatisticsCalculator.generate_data_dict(folder_paths, statistic)\n",
    "\n",
    "        # Flatten the dictionary to get a list of tuples (folder_name, file_name, stat_value)\n",
    "        flat_data = [(folder_name, file_name, stat_value) for folder_name, folder_data in data_dict.items() for file_name, stat_value in folder_data.items()]\n",
    "\n",
    "        # Sort the data alphabetically primarily by file_name and secondarily by folder_name\n",
    "        sorted_data = sorted(flat_data, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        labels = [f\"{file_name}\" for _, file_name, _ in sorted_data]\n",
    "        values = [stat_value for _, _, stat_value in sorted_data]\n",
    "        folder_names = [folder_name for folder_name, _, _ in sorted_data]\n",
    "\n",
    "        # Sort unique folder names alphabetically for consistency after regenerating\n",
    "        unique_folder_names = sorted(set(folder_names))\n",
    "\n",
    "        # Create a color map dictionary based on alphabetical order\n",
    "        color_map = {folder_name: plt.cm.tab10(i) for i, folder_name in enumerate(unique_folder_names)}\n",
    "        # Assign colors based on alphabetical order\n",
    "        colors = [color_map[folder_name] for folder_name in folder_names]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.3\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Adjust figure size as needed\n",
    "        bars = ax.bar(x, values, width, label=statistic.capitalize(), color=colors)\n",
    "\n",
    "        ax.set_ylabel(f'{statistic.capitalize()} Values')\n",
    "        ax.set_title(f'Comparison of {statistic.capitalize()}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "        # Manually create legend handles with appropriate colors\n",
    "        legend_handles = [plt.Line2D([0], [0], color=color_map[folder_name], lw=4) for folder_name in unique_folder_names]\n",
    "        legend_labels = [f'{folder}' for folder in unique_folder_names]\n",
    "\n",
    "        # Add legend with custom handles and labels\n",
    "        ax.legend(legend_handles, legend_labels)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.close()\n",
    "\n",
    "        return img_buf\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_basic_graph(data, filename):\n",
    "        # Function implementation\n",
    "        plt.figure()\n",
    "        plt.plot(data, marker='o', linestyle='-')\n",
    "        plt.title(f\"Basic Graph - {filename}\")\n",
    "        plt.xlabel(\"Query\")\n",
    "        plt.ylabel(\"Time (ms)\")\n",
    "        plt.grid(True)\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_histogram(data, filename):\n",
    "        plt.figure()\n",
    "        \n",
    "        # Square Root Rule\n",
    "        num_bins = int(np.sqrt(len(data)))\n",
    "\n",
    "        # Plot the histogram\n",
    "        plt.hist(data, bins=num_bins, color='blue', edgecolor='black', density=True, alpha=0.7, label='Data Histogram')\n",
    "\n",
    "        # Overlay normal distribution curve\n",
    "        mu, sigma = mean(data), stdev(data)\n",
    "        x = np.linspace(min(data), max(data), 100)\n",
    "        y = norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\n",
    "\n",
    "        # Overlay exponential distribution curve\n",
    "        loc, scale = min(data), stdev(data)  # Adjusted scale parameter\n",
    "        y_exponential = expon.pdf(x, loc, scale)\n",
    "        plt.plot(x, y_exponential, 'g-', linewidth=2, label='Exponential Distribution')\n",
    "        \n",
    "        plt.title(f\"Histogram - {filename}\")\n",
    "        plt.xlabel(\"Time (ms)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # show the diagram\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_overall_stats(folder_paths):\n",
    "        \n",
    "        overall_doc = docx.Document()\n",
    "        \n",
    "        # for stat_type in ['mean', 'variance', 'standard_deviation']:\n",
    "        for stat_type in ['mean', 'median', 'max', 'min', 'variance', 'standard_deviation', 'percentile_90']:\n",
    "\n",
    "            # Generate comparison chart and add it to the overall doc\n",
    "            img_buf = StatisticsGenerator.generate_comparison_chart(folder_paths, statistic=stat_type)\n",
    "            overall_doc.add_heading(f\"Comparison Chart - {stat_type.capitalize()}\", level=1)\n",
    "            overall_doc.add_picture(img_buf, width=docx.shared.Inches(7))\n",
    "            overall_doc.add_page_break()\n",
    "        \n",
    "        # overall_output_path = os.path.join(\"./\", \"overall_stats.docx\")\n",
    "        # overall_output_path = folder_paths.join(\"./\", \"overall_stats.docx\")\n",
    "        overall_output_path = os.path.join(folder_paths[0], \"overall_stats.docx\")\n",
    "        overall_doc.save(overall_output_path)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_stats_docx(folder_paths):\n",
    "        for folder_path in folder_paths:\n",
    "            doc = docx.Document()\n",
    "            doc.add_heading(f\"Statistics for Files in {folder_path}\", level=1)\n",
    "\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read().splitlines()\n",
    "\n",
    "                        # Extract numerical data from the file\n",
    "                        data = [float(line) for line in content if line.strip().replace('.', '').isdigit()]\n",
    "\n",
    "                        if data:\n",
    "                            # Basic statistics\n",
    "                            doc.add_heading(f\"File: {filename}\", level=2)\n",
    "                            doc.add_paragraph(f\"Number of queries: {round(len(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Mean: {round(np.mean(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Median: {round(np.median(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Max: {round(np.max(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Min: {round(np.min(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Variance: {round(np.var(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Standard Deviation: {round(np.std(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"90th Percentile: {round(np.percentile(data, 90), 2)}\")\n",
    "                            \n",
    "                            # Perform normality test\n",
    "                            p_value_normality = StatisticsTest.test_normality(data)\n",
    "                            doc.add_paragraph(f\"Is normal: {'Yes' if p_value_normality > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tShapiro-Wilk test p-value: {p_value_normality}\")\n",
    "\n",
    "                            # Perform exponential distribution test\n",
    "                            p_value_exponential = StatisticsTest.test_exponential(data)\n",
    "                            doc.add_paragraph(f\"Is exponential: {'Yes' if p_value_exponential > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tKolmogorov-Smirnov test p-value: {p_value_exponential}\")\n",
    "                            doc.add_paragraph(\"\\n\")\n",
    "\n",
    "                            #print out the paragraph\n",
    "                            \n",
    "\n",
    "                            # Generate basic graph\n",
    "                            img_buf_basic = StatisticsGenerator.generate_basic_graph(data, filename)\n",
    "                            doc.add_picture(img_buf_basic, width=docx.shared.Inches(6))\n",
    "\n",
    "                            # Generate histogram\n",
    "                            img_buf_histogram = StatisticsGenerator.generate_histogram(data, filename)\n",
    "                            doc.add_picture(img_buf_histogram, width=docx.shared.Inches(6))\n",
    "                            doc.add_page_break()\n",
    "\n",
    "            output_docx_path = os.path.join(folder_path, \"statistics.docx\")\n",
    "            doc.save(output_docx_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2286.702871322632], False)\n",
      "([1514.8043632507324], False)\n",
      "([1579.2045593261719], False)\n",
      "([1585.8547687530518], False)\n",
      "([1254.4865608215332], False)\n",
      "([1818.0665969848633], False)\n",
      "([2095.7858562469482], False)\n",
      "([2931.0061931610107], False)\n",
      "Query: workload_1\n",
      "Mean:  2286.702880859375 ms\n",
      "Query: workload_2\n",
      "Mean:  1514.8043212890625 ms\n",
      "Query: workload_3\n",
      "Mean:  1579.20458984375 ms\n",
      "Query: workload_4\n",
      "Mean:  1585.854736328125 ms\n",
      "Query: workload_5\n",
      "Mean:  1254.486572265625 ms\n",
      "Query: workload_6\n",
      "Mean:  1818.066650390625 ms\n",
      "Query: workload_7\n",
      "Mean:  2095.785888671875 ms\n",
      "Query: workload_8\n",
      "Mean:  2931.006103515625 ms\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    query_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Queries/gql_queries_1.json'\n",
    "    save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v1/postgres'\n",
    "    # save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v1/cockroach'\n",
    "    # save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v1/yugabyte'\n",
    "    query_performance_analyzer = QueryPerformanceAnalyzer(query_file_path)\n",
    "    await query_performance_analyzer.send_queries(100, save_file_path)\n",
    "\n",
    "\n",
    "await main()\n",
    "\n",
    "\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v1/cockroach'\n",
    "# save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v1/yugabyte'\n",
    "save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v1/postgres'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_overall_stats([save_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StatisticsGenerator.generate_stats_docx([save_file_path])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
