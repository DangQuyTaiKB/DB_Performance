{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import docx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, expon\n",
    "from statistics import mean, stdev\n",
    "from io import BytesIO\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway_url = \"http://localhost:33000/api/gql\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Define the query\n",
    "graphql_queries = {  \n",
    "   \"gql_forms\" : \"\"\"\n",
    "   {\n",
    "        formCategoryPage {\n",
    "        id\n",
    "        name\n",
    "        nameEn\n",
    "        created\n",
    "        lastchange\n",
    "        }\n",
    "    }\"\"\"\n",
    "    ,\n",
    "  \"gql_ug\" : \"\"\"\n",
    "  {\n",
    "    userPage {\n",
    "      id\n",
    "      name\n",
    "      surname\n",
    "      email\n",
    "      valid\n",
    "    }\n",
    "  }\"\"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def execute_query(query, variables):\n",
    "    data = {\"query\": query, \"variables\": variables}\n",
    "    response = requests.post(gateway_url, headers=headers, json=data)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# execute_query(graphql_queries[\"gql_forms\"], {})\n",
    "\n",
    "\n",
    "class fileHandle:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "    \n",
    "    def open(self, mode):\n",
    "        self.file = open(self.filename, mode)\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "    \n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPerformanceAnalyzer:\n",
    "    def __init__(self, query_file_path):\n",
    "        self.query_file_path = query_file_path\n",
    "        self.gateway_url = \"http://localhost:33000/api/gql\"\n",
    "        self.headers = {\"Content-Type\": \"application/json\"}\\\n",
    "    \n",
    "    \n",
    "    def runSingleTestSet(self, query, num_queries):\n",
    "        times = []\n",
    "        error = False\n",
    "\n",
    "        for _ in range(num_queries):\n",
    "            start_time = time.time()\n",
    "            response = requests.post(self.gateway_url, json=query, headers=self.headers)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times.append((end_time - start_time) * 1000)  # Convert seconds to milliseconds\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                error = True\n",
    "                break\n",
    "\n",
    "        return times, error\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def save_times_results(self, query, times, folder_path):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"{query}.txt\")\n",
    "\n",
    "        with open(file_path, mode='w') as file:\n",
    "            for time_value in times:\n",
    "                file.write(f\"{time_value}\\n\")\n",
    "\n",
    "\n",
    "    def send_queries(self, num_queries, save_file_path):\n",
    "        with open(self.query_file_path, 'r') as file:\n",
    "            graphql_queries = json.load(file)\n",
    "\n",
    "        for query, graphql_query in graphql_queries.items():\n",
    "            payload = {\"query\": graphql_query}\n",
    "            times, error = self.runSingleTestSet(payload, num_queries)\n",
    "\n",
    "\n",
    "            print(\"************ \", query, \" ************\")\n",
    "            if error:\n",
    "                print(\"ERROR\")\n",
    "                pass\n",
    "            else:\n",
    "                times.sort()\n",
    "                numpy_times = np.asarray(times, dtype=np.float32)\n",
    "\n",
    "                mean_val = np.mean(numpy_times).item()\n",
    "                variance = np.var(numpy_times).item()\n",
    "                max_val = np.max(numpy_times).item()\n",
    "                min_val = np.min(numpy_times).item()\n",
    "                median_val = np.median(numpy_times).item()\n",
    "                stdev_val = np.std(numpy_times).item()\n",
    "                percentile_90 = np.percentile(numpy_times, 90).item()\n",
    "\n",
    "\n",
    "                print(\"Mean: \", mean_val, \"ms\")\n",
    "                # print(\"Variance: \", variance)\n",
    "                # print(\"Max: \", max_val, \"ms\")\n",
    "                # print(\"Min: \", min_val, \"ms\")\n",
    "                # print(\"Median: \", median_val, \"ms\")\n",
    "                # print(\"Standard Deviation: \", stdev_val)\n",
    "                # print(\"90th Percentile: \", percentile_90, \"ms\")\n",
    "\n",
    "                self.save_times_results(query, times, save_file_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "    def runStressTest(self, num_queries):\n",
    "        with open(self.query_file_path, 'r') as file:\n",
    "            graphql_queries = json.load(file)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for query, graphql_query in graphql_queries.items():\n",
    "                for _ in range(num_queries):\n",
    "                    futures.append(executor.submit(self.runSingleTestSet, {\"query\": graphql_query}, num_queries))\n",
    "\n",
    "            results = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                times, error = future.result()\n",
    "                if error:\n",
    "                    print(\"ERROR\")\n",
    "                else:\n",
    "                    results.append(times)\n",
    "\n",
    "        return results\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsTest:\n",
    "    @staticmethod\n",
    "    def test_normality(data):\n",
    "        _, p_value = stats.shapiro(data)\n",
    "        return p_value\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def test_exponential(data):\n",
    "        loc_estimate = min(data)\n",
    "        scale_estimate = 1 / np.mean(data)\n",
    "        _, p_value = stats.kstest(data, 'expon', args=(loc_estimate, scale_estimate))\n",
    "        return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticsCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_statistic(file_path):\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            query_times = [float(line.strip()) for line in file]\n",
    "\n",
    "        mean_val = np.mean(query_times)\n",
    "        median_val = np.median(query_times)\n",
    "        max_val = np.max(query_times)\n",
    "        min_val = np.min(query_times)\n",
    "        variance = np.var(query_times)\n",
    "        std_dev = np.std(query_times)\n",
    "        percentile_90 = np.percentile(query_times, 90)\n",
    "        return mean_val, median_val, max_val, min_val, variance, std_dev, percentile_90\n",
    "    \n",
    "    def generate_data_dict(folder_paths, statistic='mean'):\n",
    "        data_dict = {}\n",
    "\n",
    "        for folder_path in folder_paths:\n",
    "            folder_data = {}\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    stats = StatisticsCalculator.calculate_statistic(file_path)\n",
    "                    stats_dict = {\n",
    "                        'mean': stats[0],\n",
    "                        'median': stats[1],\n",
    "                        'max': stats[2],\n",
    "                        'min': stats[3],\n",
    "                        'variance': stats[4],\n",
    "                        'standard_deviation': stats[5],\n",
    "                        'percentile_90': stats[6]\n",
    "                    }\n",
    "\n",
    "                    folder_data[os.path.splitext(file_name)[0]] = stats_dict.get(statistic)\n",
    "\n",
    "            folder_name = os.path.basename(folder_path).replace('queries_times_', '')\n",
    "            data_dict[folder_name] = folder_data\n",
    "\n",
    "        return data_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class StatisticsGenerator:\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_comparison_chart(folder_paths, statistic='mean'):\n",
    "        data_dict = StatisticsCalculator.generate_data_dict(folder_paths, statistic)\n",
    "\n",
    "        # Flatten the dictionary to get a list of tuples (folder_name, file_name, stat_value)\n",
    "        flat_data = [(folder_name, file_name, stat_value) for folder_name, folder_data in data_dict.items() for file_name, stat_value in folder_data.items()]\n",
    "\n",
    "        # Sort the data alphabetically primarily by file_name and secondarily by folder_name\n",
    "        sorted_data = sorted(flat_data, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        labels = [f\"{file_name}\" for _, file_name, _ in sorted_data]\n",
    "        values = [stat_value for _, _, stat_value in sorted_data]\n",
    "        folder_names = [folder_name for folder_name, _, _ in sorted_data]\n",
    "\n",
    "        # Sort unique folder names alphabetically for consistency after regenerating\n",
    "        unique_folder_names = sorted(set(folder_names))\n",
    "\n",
    "        # Create a color map dictionary based on alphabetical order\n",
    "        color_map = {folder_name: plt.cm.tab10(i) for i, folder_name in enumerate(unique_folder_names)}\n",
    "        # Assign colors based on alphabetical order\n",
    "        colors = [color_map[folder_name] for folder_name in folder_names]\n",
    "\n",
    "        x = np.arange(len(labels))\n",
    "        width = 0.3\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))  # Adjust figure size as needed\n",
    "        bars = ax.bar(x, values, width, label=statistic.capitalize(), color=colors)\n",
    "\n",
    "        ax.set_ylabel(f'{statistic.capitalize()} Values')\n",
    "        ax.set_title(f'Comparison of {statistic.capitalize()}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "        # Manually create legend handles with appropriate colors\n",
    "        legend_handles = [plt.Line2D([0], [0], color=color_map[folder_name], lw=4) for folder_name in unique_folder_names]\n",
    "        legend_labels = [f'{folder}' for folder in unique_folder_names]\n",
    "\n",
    "        # Add legend with custom handles and labels\n",
    "        ax.legend(legend_handles, legend_labels)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png', dpi=300, bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.close()\n",
    "\n",
    "        return img_buf\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_basic_graph(data, filename):\n",
    "        # Function implementation\n",
    "        plt.figure()\n",
    "        plt.plot(data, marker='o', linestyle='-')\n",
    "        plt.title(f\"Basic Graph - {filename}\")\n",
    "        plt.xlabel(\"Query\")\n",
    "        plt.ylabel(\"Time (ms)\")\n",
    "        plt.grid(True)\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_histogram(data, filename):\n",
    "        plt.figure()\n",
    "        \n",
    "        # Square Root Rule\n",
    "        num_bins = int(np.sqrt(len(data)))\n",
    "\n",
    "        # Plot the histogram\n",
    "        plt.hist(data, bins=num_bins, color='blue', edgecolor='black', density=True, alpha=0.7, label='Data Histogram')\n",
    "\n",
    "        # Overlay normal distribution curve\n",
    "        mu, sigma = mean(data), stdev(data)\n",
    "        x = np.linspace(min(data), max(data), 100)\n",
    "        y = norm.pdf(x, mu, sigma)\n",
    "        plt.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')\n",
    "\n",
    "        # Overlay exponential distribution curve\n",
    "        loc, scale = min(data), stdev(data)  # Adjusted scale parameter\n",
    "        y_exponential = expon.pdf(x, loc, scale)\n",
    "        plt.plot(x, y_exponential, 'g-', linewidth=2, label='Exponential Distribution')\n",
    "        \n",
    "        plt.title(f\"Histogram - {filename}\")\n",
    "        plt.xlabel(\"Time (ms)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the plot to a BytesIO object\n",
    "        img_buf = BytesIO()\n",
    "        plt.savefig(img_buf, format='png')\n",
    "        plt.close()\n",
    "\n",
    "        # Return the BytesIO object\n",
    "        return img_buf\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_overall_stats(folder_paths):\n",
    "        \n",
    "        overall_doc = docx.Document()\n",
    "        \n",
    "        # for stat_type in ['mean', 'variance', 'standard_deviation']:\n",
    "        for stat_type in ['mean', 'median', 'max', 'min', 'variance', 'standard_deviation', 'percentile_90']:\n",
    "\n",
    "            # Generate comparison chart and add it to the overall doc\n",
    "            img_buf = StatisticsGenerator.generate_comparison_chart(folder_paths, statistic=stat_type)\n",
    "            overall_doc.add_heading(f\"Comparison Chart - {stat_type.capitalize()}\", level=1)\n",
    "            overall_doc.add_picture(img_buf, width=docx.shared.Inches(7))\n",
    "            overall_doc.add_page_break()\n",
    "        \n",
    "        # overall_output_path = os.path.join(\"./\", \"overall_stats.docx\")\n",
    "        # overall_output_path = folder_paths.join(\"./\", \"overall_stats.docx\")\n",
    "        overall_output_path = os.path.join(folder_paths[0], \"overall_stats.docx\")\n",
    "        overall_doc.save(overall_output_path)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_stats_docx(folder_paths):\n",
    "        for folder_path in folder_paths:\n",
    "            doc = docx.Document()\n",
    "            doc.add_heading(f\"Statistics for Files in {folder_path}\", level=1)\n",
    "\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        content = file.read().splitlines()\n",
    "\n",
    "                        # Extract numerical data from the file\n",
    "                        data = [float(line) for line in content if line.strip().replace('.', '').isdigit()]\n",
    "\n",
    "                        if data:\n",
    "                            # Basic statistics\n",
    "                            doc.add_heading(f\"File: {filename}\", level=2)\n",
    "                            doc.add_paragraph(f\"Number of queries: {round(len(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Mean: {round(np.mean(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Median: {round(np.median(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Max: {round(np.max(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Min: {round(np.min(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Variance: {round(np.var(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"Standard Deviation: {round(np.std(data), 2)}\")\n",
    "                            doc.add_paragraph(f\"90th Percentile: {round(np.percentile(data, 90), 2)}\")\n",
    "                            \n",
    "                            # Perform normality test\n",
    "                            p_value_normality = StatisticsTest.test_normality(data)\n",
    "                            doc.add_paragraph(f\"Is normal: {'Yes' if p_value_normality > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tShapiro-Wilk test p-value: {p_value_normality}\")\n",
    "\n",
    "                            # Perform exponential distribution test\n",
    "                            p_value_exponential = StatisticsTest.test_exponential(data)\n",
    "                            doc.add_paragraph(f\"Is exponential: {'Yes' if p_value_exponential > 0.05 else 'No'}\")\n",
    "                            doc.add_paragraph(f\"\\tKolmogorov-Smirnov test p-value: {p_value_exponential}\")\n",
    "                            doc.add_paragraph(\"\\n\")\n",
    "\n",
    "                            #print out the paragraph\n",
    "                            \n",
    "\n",
    "                            # Generate basic graph\n",
    "                            img_buf_basic = StatisticsGenerator.generate_basic_graph(data, filename)\n",
    "                            doc.add_picture(img_buf_basic, width=docx.shared.Inches(6))\n",
    "\n",
    "                            # Generate histogram\n",
    "                            img_buf_histogram = StatisticsGenerator.generate_histogram(data, filename)\n",
    "                            doc.add_picture(img_buf_histogram, width=docx.shared.Inches(6))\n",
    "                            doc.add_page_break()\n",
    "\n",
    "            output_docx_path = os.path.join(folder_path, \"statistics.docx\")\n",
    "            doc.save(output_docx_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************  gql_ug  ************\n",
      "Mean:  62.34349822998047 ms\n",
      "\n",
      "************  gql_facilities  ************\n",
      "ERROR\n",
      "\n",
      "************  gql_forms  ************\n",
      "Mean:  20.25470733642578 ms\n",
      "\n",
      "************  gql_lessons  ************\n",
      "Mean:  25.718069076538086 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Queries/gql_queries.json'\n",
    "save_file_path = 'D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Test_v4'\n",
    "folder_paths = ['D:/Documents/Unob_7/STC/STC_code/DB_Performance/_Test/Performance/Result/cockroach']\n",
    "\n",
    "\n",
    "query_performance_analyzer = QueryPerformanceAnalyzer(query_file_path)\n",
    "query_performance_analyzer.send_queries(5, save_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_overall_stats([save_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "StatisticsGenerator.generate_stats_docx([save_file_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def execute_query(query, variables):\n",
    "#     data = {\"query\": query, \"variables\": variables}\n",
    "#     response = requests.post(gateway_url, headers=headers, json=data)\n",
    "#     return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = query_performance_analyzer.runStressTest(3)\n",
    "# print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
